{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import gaussian_kde\n",
    "sys.path.append('vbpi-torch/rooted')\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "\n",
    "import torch\n",
    "from dataManipulation import *\n",
    "from treeManipulation import *\n",
    "from utils import tree_summary, summary, summary_raw, get_support_info\n",
    "from vbpi import VBPI\n",
    "from VIPR import VIPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")  # Options: white, dark, whitegrid, darkgrid, ticks\n",
    "sns.set_palette(\"muted\")    # Options: deep, muted, bright, pastel, dark, colorblind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "from io import StringIO\n",
    "from Bio import Phylo\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = \"DS1\"\n",
    "pop_size = 5.0 # exponential parameter for constant pop size prior\n",
    "var_dist = \"LogNormal\"\n",
    "\n",
    "# initialize models\n",
    "models = {\"reinforce\": {},\n",
    "          \"reparam\": {},\n",
    "          \"VIMCO\": {},\n",
    "          \"VBPI\": {}}\n",
    "\n",
    "data_file = '../dat/'+data_set+'/'+data_set+'.pickle'\n",
    "\n",
    "# models\n",
    "models = [\"reparam\",\"reinforce\",\"VIMCO\",\"BEAST\",\"VBPI\"]\n",
    "\n",
    "# Beast file\n",
    "BEAST_pref = '../dat/'+data_set+'/'+data_set+'_MLL_'\n",
    "BEAST_burnin = 250000\n",
    "\n",
    "# VPBI files\n",
    "VBPI_dir = '../results/'+data_set+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in VBPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sequence data and estimate the subsplit support\n",
    "data, taxa = loadData('../dat/'+data_set+'/'+data_set+'.nexus', 'nexus')\n",
    "mcmc_support_trees_dict, mcmc_support_trees_wts = summary('../dat/'+data_set+'/'+data_set+'_fixed_pop_support_short_run', 'nexus', burnin=250)\n",
    "rootsplit_supp_dict, subsplit_supp_dict = get_support_info(taxa, mcmc_support_trees_dict)\n",
    "emp_tree_freq = None\n",
    "sample_info = [0.0 for taxon in taxa]\n",
    "\n",
    "VBPI_models = {}\n",
    "VBPI_models[\"10\"] = VBPI(taxa, rootsplit_supp_dict, subsplit_supp_dict, data, pden=np.ones(4)/4., subModel=('JC', 1.0),\n",
    "             emp_tree_freq=emp_tree_freq, root_height_offset=0.0, clock_rate=1.0, psp=True,\n",
    "             sample_info=sample_info, coalescent_type='fixed_pop', clock_type='fixed_rate',\n",
    "             log_pop_size_offset=math.log(5.0))\n",
    "\n",
    "VBPI_models[\"20\"] = VBPI(taxa, rootsplit_supp_dict, subsplit_supp_dict, data, pden=np.ones(4)/4., subModel=('JC', 1.0),\n",
    "             emp_tree_freq=emp_tree_freq, root_height_offset=0.0, clock_rate=1.0, psp=True,\n",
    "             sample_info=sample_info, coalescent_type='fixed_pop', clock_type='fixed_rate',\n",
    "             log_pop_size_offset=math.log(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the best VBPI10 and VBPI20 model \n",
    "\n",
    "VBPI_runtimes = {}\n",
    "VBPI_lbs = {}\n",
    "VBPI_iters = {}\n",
    "\n",
    "legend = []\n",
    "\n",
    "for bs in [\"10\",\"20\"]:\n",
    "    \n",
    "    VBPI_runtimes[bs] = None\n",
    "    VBPI_lbs[bs] = None\n",
    "    VBPI_iters[bs] = None\n",
    "    lb_star = -np.inf\n",
    "    file_star = None\n",
    "    \n",
    "    for ss in [\"0.003\",\"0.001\",\"0.0003\",\"0.0001\"]:\n",
    "        VBPI_pref = \"mcmc_vimco_%s_%s_psp_fixed_pop_fixed_rate_\"%(bs,ss)\n",
    "        files = [x for x in os.listdir(VBPI_dir) if x.startswith(VBPI_pref)]\n",
    "        files = [x for x in files if x.endswith(\".pt\")]\n",
    "        new_files = []\n",
    "\n",
    "        for x in files:\n",
    "            new_files.append(x)\n",
    "        \n",
    "        files = new_files\n",
    "        \n",
    "        for file in files:\n",
    "        \n",
    "            VBPI_runtimes0 = np.load(VBPI_dir+file.replace(\".pt\",\"_run_time.npy\"))\n",
    "            VBPI_lbs0 = np.load(VBPI_dir+file.replace(\".pt\",\"_test_lb.npy\"))\n",
    "            VBPI_iters0 = np.load(VBPI_dir+file.replace(\".pt\",\"_iters.npy\"))\n",
    "\n",
    "            if np.mean(VBPI_lbs0[-10:]) > lb_star:\n",
    "                try:\n",
    "                    VBPI_runtimes[bs] = VBPI_runtimes0\n",
    "                    VBPI_lbs[bs] = VBPI_lbs0\n",
    "                    VBPI_iters[bs] = VBPI_iters0\n",
    "                    VBPI_models[bs].load_from(VBPI_dir+file)\n",
    "                    lb_star = np.mean(VBPI_lbs0[-10:])\n",
    "                    file_star = file\n",
    "                except:\n",
    "                    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in BEAST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract BEAST data\n",
    "def load_beast(data_set,i,burnin):\n",
    "    df = pd.read_csv('../dat/'+data_set+'/'+data_set+'_fixed_pop_MLL_%d.log'%i, \n",
    "                     sep = '\\t',skiprows=[0,1,2])\n",
    "    df = df[df.state > burnin]\n",
    "    return df\n",
    "\n",
    "BEAST_data = pd.concat([load_beast(data_set,i,BEAST_burnin) for i in range(1,11)])\n",
    "BEAST_MLLs = []\n",
    "\n",
    "# extract MLL from beast log\n",
    "for i in range(1,11):\n",
    "    with open('../dat/'+data_set+'/'+data_set+\"_MLL_%d.txt\"%i, \"r\") as text_file:\n",
    "        line = text_file.readlines()[-4]\n",
    "    ind = np.where([not x in \"-1234567890.\" for x in line])[0][-2]\n",
    "    BEAST_MLLs.append(float(line[(ind+1):-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in VIPR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in VIPR models\n",
    "optims = {}\n",
    "settings = {}\n",
    "ELBO_star = -np.inf\n",
    "fname_star = None\n",
    "ELBO_min = np.inf\n",
    "ELBO_max = -np.inf\n",
    "\n",
    "for model in [\"reinforce\",\"reparam\",\"VIMCO\"]:\n",
    "\n",
    "    print(model)\n",
    "    optims[model] = None\n",
    "    ELBO_star = -np.inf\n",
    "    \n",
    "    for ss in [0.03,0.01,0.003,0.001]:\n",
    "    \n",
    "        for rs in range(5):\n",
    "            \n",
    "            optim_dir = '../results/'+data_set+'/'\n",
    "            optim_pref = data_set+'_'+var_dist+'_'+model+'_'+str(ss)+'_'+str(rs)\n",
    "            files = [x for x in os.listdir(optim_dir) if x.startswith(optim_pref)]\n",
    "\n",
    "            if not files:\n",
    "                continue\n",
    "            \n",
    "            fname = optim_dir + max(files)\n",
    "    \n",
    "            with open(fname, 'rb') as f:\n",
    "                optim0 = pickle.load(f)\n",
    "            \n",
    "            if np.mean(optim0.multi_ELBO_ests[-10:]) > ELBO_star:\n",
    "                optims[model] = optim0\n",
    "                settings[model] = (ss,rs)\n",
    "                ELBO_star = np.mean(optim0.multi_ELBO_ests[-10:])\n",
    "                fname_star = fname\n",
    "    \n",
    "    if max(optims[model].multi_ELBO_ests) > ELBO_max:\n",
    "        ELBO_max = max(optims[model].multi_ELBO_ests)\n",
    "\n",
    "    if min(optims[model].multi_ELBO_ests) < ELBO_min:\n",
    "        ELBO_min = min(optims[model].multi_ELBO_ests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot estimated ELBO over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = []\n",
    "for model in [\"reinforce\",\"reparam\",\"VIMCO\"]:\n",
    "    plt.plot(np.cumsum(optims[model].run_times)/3600,optims[model].multi_ELBO_ests)\n",
    "    legend.append(model+\" \"+str(settings[model][0])+\" \"+str(settings[model][1]))\n",
    "\n",
    "for bs in [\"10\",\"20\"]:\n",
    "    ELBO_max = max(ELBO_max,max(VBPI_lbs[bs]))\n",
    "    plt.plot(np.cumsum(VBPI_runtimes[bs])/3600,VBPI_lbs[bs])\n",
    "    legend.append(\"Baseline VIMCO, K = %s\"%(bs))  \n",
    "\n",
    "plt.xlabel(\"runtime (hours)\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend(legend)\n",
    "plt.savefig('../plt/'+data_set+'/'+data_set+'_ELBO_v_time.png')\n",
    "plt.savefig('../plt/'+data_set+'/'+data_set+'_ELBO_v_time_zoom.png')\n",
    "plt.show()\n",
    "\n",
    "##############\n",
    "\n",
    "legend = []\n",
    "for model in [\"reinforce\",\"reparam\",\"VIMCO\"]:\n",
    "    plt.plot(optims[model].epochs,optims[model].multi_ELBO_ests)\n",
    "    legend.append(model+\" \"+str(settings[model][0])+\" \"+str(settings[model][1]))\n",
    "\n",
    "for bs in [\"10\",\"20\"]:\n",
    "    plt.plot(VBPI_iters[bs],VBPI_lbs[bs])\n",
    "    legend.append(\"Baseline VIMCO, K = %s\"%(bs))  \n",
    "\n",
    "plt.xlabel(\"Gradient Evaluations\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.legend(legend)\n",
    "\n",
    "plt.xscale('log')\n",
    "    \n",
    "#plt.ylim([ELBO_min-10,ELBO_max+10])\n",
    "plt.savefig('../plt/'+data_set+'/'+data_set+'_ELBO_v_iter.png')\n",
    "#plt.ylim([ELBO_max-20,ELBO_max+10])\n",
    "plt.savefig('../plt/'+data_set+'/'+data_set+'_ELBO_v_iter_zoom.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_lengths = {}\n",
    "root_heights = {}\n",
    "log_likes = {}\n",
    "log_priors = {}\n",
    "p_qs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from BEAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lengths from BEAST\n",
    "tree_lengths[\"BEAST\"] = BEAST_data.treeLength[BEAST_data.state > BEAST_burnin].to_numpy()\n",
    "root_heights[\"BEAST\"] = BEAST_data['treeModel.rootHeight'][BEAST_data.state > BEAST_burnin].to_numpy()\n",
    "log_likes[\"BEAST\"] = BEAST_data.likelihood[BEAST_data.state > BEAST_burnin].to_numpy()\n",
    "log_priors[\"BEAST\"] = BEAST_data.prior[BEAST_data.state > BEAST_burnin].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from VBPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bs in [\"10\",\"20\"]:\n",
    "    self = VBPI_models[bs]\n",
    "    n_runs = 1000\n",
    "    n_particles = 1\n",
    "\n",
    "    root_heights[\"VBPI_%s\"%bs] = []\n",
    "    tree_lengths[\"VBPI_%s\"%bs] = []\n",
    "    log_priors[\"VBPI_%s\"%bs] = []\n",
    "    log_likes[\"VBPI_%s\"%bs] = []\n",
    "    p_qs[\"VBPI_%s\"%bs] = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        with torch.no_grad():\n",
    "            samp_trees = [self.tree_model.sample_tree() for particle in range(n_particles)]\n",
    "            [namenum(tree, self.taxa) for tree in samp_trees]\n",
    "            logq_tree = torch.stack([self.logq_tree(tree) for tree in samp_trees])\n",
    "\n",
    "            samp_branch, logq_height, height, event_info = self.branch_model(samp_trees)\n",
    "            log_clock_rate, logq_clock_rate = self.clock_model.sample(n_particles=n_particles)\n",
    "            samp_branch = samp_branch.to(torch.float32) * log_clock_rate.exp()\n",
    "            logll = torch.stack([self.phylo_model.loglikelihood(branch, tree) for branch, tree in zip(*[samp_branch, samp_trees])])\n",
    "\n",
    "            self.tree_prior_model.update_batch(height, event_info)\n",
    "            coalescent_param, logq_prior = self.tree_prior_model.sample_pop_size(n_particles=n_particles)\n",
    "            logp_coalescent_prior, _ = self.tree_prior_model(coalescent_param, False)\n",
    "\n",
    "            logp_clock_rate = self.clock_model(log_clock_rate)\n",
    "\n",
    "            # get values\n",
    "            root_heights[\"VBPI_%s\"%bs].extend(list(height[:,0].numpy()))\n",
    "            tree_lengths[\"VBPI_%s\"%bs].extend(list(np.sum(samp_branch.numpy(),axis=1)))\n",
    "            log_priors[\"VBPI_%s\"%bs].extend(list(logp_coalescent_prior.numpy() + logp_clock_rate))\n",
    "            log_likes[\"VBPI_%s\"%bs].extend(list(logll.numpy()))\n",
    "            p_qs[\"VBPI_%s\"%bs].append((logll + logp_coalescent_prior + logp_clock_rate - logq_tree - logq_height - logq_prior - logq_clock_rate - math.log(n_particles)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from VIPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_exp = 1000\n",
    "\n",
    "for model in [\"reinforce\",\"reparam\",\"VIMCO\"]:\n",
    "    with torch.no_grad():\n",
    "        phi = optims[model].phi\n",
    "\n",
    "        #optims[model].var_dist = \"LogNormal\"\n",
    "\n",
    "        root_heights[model] = []\n",
    "        tree_lengths[model] = []\n",
    "        log_priors[model] = []\n",
    "        log_likes[model] = []\n",
    "        p_qs[model] = []\n",
    "\n",
    "        for i in range(n_exp):\n",
    "            \n",
    "            if i % int(n_exp/10) == 0:\n",
    "                print(i/n_exp)\n",
    "\n",
    "            # sample from variational distribution\n",
    "            pop_size = optims[model].sample_q_pop_size()\n",
    "            rate = optims[model].sample_q_rate()\n",
    "            Z = optims[model].sample_q()\n",
    "\n",
    "            # evaluate joint distribution\n",
    "            log_prior = optims[model].logprior(Z,pop_size) + optims[model].logprior_pop_size(pop_size) + optims[model].logprior_rate(rate)\n",
    "            log_ll = optims[model].loglikelihood(Z,rate)\n",
    "\n",
    "            # evaluate variational distribution\n",
    "            log_q = optims[model].logvariational(Z) + optims[model].logvariational_pop_size(pop_size) + optims[model].logvariational_rate(rate)\n",
    "\n",
    "            # get tree and branches from Z\n",
    "            tree,branches = optims[model].scipy_linkage_to_ete3(Z)\n",
    "\n",
    "            p_qs[model].append(log_prior.item() + log_ll.item() - log_q.item())\n",
    "            log_likes[model].append(log_ll.item())\n",
    "            log_priors[model].append(log_prior.item())\n",
    "            root_heights[model].append(Z[-1,2].item())\n",
    "            tree_lengths[model].append(sum([node.branch for node in tree.traverse() if node.branch]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate Figure 9 from Zhang et al (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(6, 6), dpi=600, \n",
    "                         gridspec_kw={'height_ratios': [1, 1]})\n",
    "\n",
    "colors = {\"BEAST\":plt.cm.tab10(0),\n",
    "          \"VBPI_10\":plt.cm.tab10(1),\n",
    "          \"VBPI_20\":plt.cm.tab10(2),\n",
    "          \"reinforce\":plt.cm.tab10(3),\n",
    "          \"reparam\":plt.cm.tab10(4),\n",
    "          \"VIMCO\":plt.cm.tab10(5)}\n",
    "\n",
    "keys = list([\"BEAST\",\"VBPI_10\",\"VBPI_20\",\"reinforce\",\"reparam\",\"VIMCO\"])\n",
    "\n",
    "### ELBO over time ###\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "plt.plot(VBPI_iters[\"10\"],\n",
    "         VBPI_lbs[\"10\"],\n",
    "         color = colors[\"VBPI_10\"])\n",
    "\n",
    "plt.plot(VBPI_iters[\"20\"],\n",
    "         VBPI_lbs[\"20\"],\n",
    "         color = colors[\"VBPI_20\"])\n",
    "\n",
    "plt.plot(optims[\"reinforce\"].epochs,\n",
    "         optims[\"reinforce\"].multi_ELBO_ests,\n",
    "         color = colors[\"reinforce\"])\n",
    "\n",
    "plt.plot(optims[\"reparam\"].epochs,\n",
    "         optims[\"reparam\"].multi_ELBO_ests,\n",
    "         color = colors[\"reparam\"])\n",
    "\n",
    "plt.plot(optims[\"VIMCO\"].epochs,\n",
    "         optims[\"VIMCO\"].multi_ELBO_ests,\n",
    "         color = colors[\"VIMCO\"])\n",
    "\n",
    "plt.xlabel(\"Iterations\",fontsize = 12)\n",
    "plt.ylabel(\"MLL estimates\",fontsize = 12)\n",
    "\n",
    "ELBO_max = max(max(VBPI_lbs[\"10\"]),\n",
    "               max(optims[\"reinforce\"].multi_ELBO_ests),\n",
    "               max(optims[\"reparam\"].multi_ELBO_ests),\n",
    "               max(optims[\"VIMCO\"].multi_ELBO_ests))\n",
    "ELBO_min = min(min(VBPI_lbs[\"10\"]),\n",
    "               min(optims[\"reinforce\"].multi_ELBO_ests),\n",
    "               min(optims[\"reparam\"].multi_ELBO_ests),\n",
    "               min(optims[\"VIMCO\"].multi_ELBO_ests))\n",
    "\n",
    "plt.ylim(min(min(optims[\"reinforce\"].multi_ELBO_ests),\n",
    "             max(VBPI_lbs[\"10\"]))-5,\n",
    "         ELBO_max+5)\n",
    "plt.legend([\"VBPI_10\",\"VBPI_20\",\"LOOR\",\"Reparam\",\"VIMCO\"],prop={'size': 8})\n",
    "plt.xscale('log')\n",
    "zoom_region = (1000, 300000)  # Specify the region to zoom in\n",
    "plt.gca().add_patch(plt.Rectangle((zoom_region[0], ELBO_max-5), zoom_region[1] - zoom_region[0], 6,\n",
    "                                   edgecolor=\"black\", facecolor=\"none\", lw=2))\n",
    "\n",
    "# Define the inset\n",
    "axins = inset_axes(plt.gca(), width=\"30%\", height=\"30%\", loc=\"lower center\")\n",
    "axins.plot(VBPI_iters[\"10\"],\n",
    "           VBPI_lbs[\"10\"],\n",
    "           color = colors[\"VBPI_10\"])\n",
    "\n",
    "axins.plot(VBPI_iters[\"20\"],\n",
    "           VBPI_lbs[\"20\"],\n",
    "           color = colors[\"VBPI_20\"])\n",
    "\n",
    "axins.plot(optims[\"reinforce\"].epochs,\n",
    "           optims[\"reinforce\"].multi_ELBO_ests,\n",
    "           color = colors[\"reinforce\"])\n",
    "\n",
    "axins.plot(optims[\"reparam\"].epochs,\n",
    "         optims[\"reparam\"].multi_ELBO_ests,\n",
    "         color = colors[\"reparam\"])\n",
    "\n",
    "axins.plot(optims[\"VIMCO\"].epochs,\n",
    "         optims[\"VIMCO\"].multi_ELBO_ests,\n",
    "         color = colors[\"VIMCO\"])\n",
    "\n",
    "# Set inset limits and labels\n",
    "axins.set_xlim(zoom_region)\n",
    "axins.set_xscale('log')\n",
    "axins.set_xticklabels([])\n",
    "axins.set_ylim([ELBO_max-5,ELBO_max+1])\n",
    "\n",
    "\n",
    "# Add a rectangle to indicate the zoomed region\n",
    "plt.gca().indicate_inset_zoom(axins, edgecolor=\"black\", lw=5)\n",
    "\n",
    "\n",
    "### Tree lengths ###\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "data = {k: tree_lengths[k] for k in [\"BEAST\",\"VBPI_10\",\"VBPI_20\",\"reinforce\",\"reparam\",\"VIMCO\"]}\n",
    "title = \"Tree Lengths\"\n",
    "i = 0\n",
    "\n",
    "_, bins, _ = plt.hist(data[keys[0]], bins = 100, edgecolor='black', alpha = 0.25, \n",
    "                      density=True, color=colors[keys[0]])\n",
    "density = gaussian_kde(data[keys[0]])\n",
    "density_vals = density(bins)\n",
    "plt.plot(bins, density_vals, linestyle='-', color=colors[keys[0]])\n",
    "\n",
    "for key in keys[1:]:\n",
    "    plt.hist(data[key], bins = bins, edgecolor='black', alpha = 0.25, \n",
    "             density=True, color=colors[key])\n",
    "    density = gaussian_kde(data[key])\n",
    "    density_vals = density(bins)\n",
    "    plt.plot(bins, density_vals, linestyle='-', color=colors[key])\n",
    "\n",
    "plt.legend([\"BEAST\",\"VBPI_10\",\"VBPI_20\",\"LOOR\",\"Reparam\",\"VIMCO\"],prop={'size': 8})\n",
    "\n",
    "plt.title(title)\n",
    "plt.ylabel(\"Density\",fontsize = 12)\n",
    "\n",
    "### log-likelihooods ###\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "data = {k: log_likes[k] for k in [\"BEAST\",\"VBPI_10\",\"VBPI_20\",\"reinforce\",\"reparam\",\"VIMCO\"]}\n",
    "title = \"Tree Log-likelihood\"\n",
    "i = 1\n",
    "\n",
    "_, bins, _ = plt.hist(data[keys[0]], bins = 100, edgecolor='black', alpha = 0.25, \n",
    "                      density=True, color=colors[keys[0]])\n",
    "density = gaussian_kde(data[keys[0]])\n",
    "density_vals = density(bins)\n",
    "plt.plot(bins, density_vals, linestyle='-', color=colors[keys[0]])\n",
    "\n",
    "for key in keys[1:]:\n",
    "    plt.hist(data[key], bins = bins, edgecolor='black', alpha = 0.25, \n",
    "             density=True, color=colors[key])\n",
    "    density = gaussian_kde(data[key])\n",
    "    density_vals = density(bins)\n",
    "    plt.plot(bins, density_vals, linestyle='-', color=colors[key])\n",
    "\n",
    "plt.legend([\"BEAST\",\"VBPI_10\",\"VBPI_20\",\"LOOR\",\"Reparam\",\"VIMCO\"],prop={'size': 8})\n",
    "\n",
    "plt.title(title)\n",
    "#plt.ylabel(\"Density\")\n",
    "\n",
    "fig.delaxes(axes[1, 0])\n",
    "fig.delaxes(axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../plt/'+data_set+'/'+data_set+'_hist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bootstrap standard error of our models\n",
    "def ELBO_se(p_qs):\n",
    "    \n",
    "    ELBO_hats = []\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        boot_sample = np.random.choice(p_qs,size=len(p_qs),replace=True)\n",
    "        boot_ELBO = np.mean(boot_sample)\n",
    "        ELBO_hats.append(boot_ELBO)\n",
    "        \n",
    "    return np.std(ELBO_hats,ddof=1)\n",
    "\n",
    "def MLL_se(p_qs):\n",
    "    \n",
    "    MLL_hats = []\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        boot_sample = np.random.choice(p_qs,size=len(p_qs),replace=True)\n",
    "        boot_MLL = logsumexp(boot_sample) - np.log(len(boot_sample))\n",
    "        MLL_hats.append(boot_MLL)\n",
    "        \n",
    "    return np.std(MLL_hats,ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'model': [\"BEAST\"] + [key for key in p_qs], \n",
    "        'MLL': [np.mean(BEAST_MLLs)] + [logsumexp(p_qs[key]) - np.log(len(p_qs[key])) for key in p_qs],\n",
    "        'MLL_se': [np.std(BEAST_MLLs,ddof=1)] + [MLL_se(p_qs[key]) for key in p_qs],\n",
    "        'ELBO': [np.nan] + [np.mean(p_qs[key]) for key in p_qs],\n",
    "        'ELBO_se': [np.nan] + [ELBO_se(p_qs[key]) for key in p_qs]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"MLLs:\")\n",
    "print(\"\")\n",
    "print(\"BEAST: %s (%s)\"%(data['MLL'][0],data['MLL_se'][0]))\n",
    "for i,key in enumerate(p_qs):\n",
    "    print(\"%s: %s (%s)\"%(key,data['MLL'][i+1]-data['MLL'][0],data['MLL_se'][i+1]))\n",
    "\n",
    "print(\"\")\n",
    "print(\"ELBO:\")\n",
    "print(\"\")\n",
    "for i,key in enumerate(p_qs):\n",
    "    print(\"%s: %s (%s)\"%(key,data['ELBO'][i+1],data['ELBO_se'][i+1]))\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../plt/'+data_set+'/'+data_set+'_MLLs.csv', index=False)\n",
    "\n",
    "plt.scatter(data[\"model\"],data[\"MLL\"],c=[colors[key] for key in colors],s=500)\n",
    "plt.ylabel(\"Estimated MLL (100 samples)\")\n",
    "plt.savefig('../plt/'+data_set+'/'+data_set+'_MLLs.png')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(data[\"model\"],data[\"ELBO\"],c=[colors[key] for key in colors],s=500)\n",
    "plt.ylabel(\"Estimated ELBO (100 samples)\")\n",
    "plt.savefig('../plt/'+data_set+'/'+data_set+'_ELBOs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phi in [optims[\"reparam\"].phi,\n",
    "              optims[\"reinforce\"].phi,\n",
    "              optims[\"VIMCO\"].phi]:\n",
    "    \n",
    "    m = deepcopy(phi[0].detach())\n",
    "    m[np.triu_indices(m.shape[0])] = np.nan\n",
    "    plt.imshow(m)\n",
    "    plt.title(\"means of log-coalscent times\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    log_sig = deepcopy(phi[1].detach())\n",
    "    log_sig[np.triu_indices(log_sig.shape[0])] = np.nan\n",
    "    plt.imshow(log_sig)\n",
    "    plt.title(\"log-std of log-coalscent times\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
